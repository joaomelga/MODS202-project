{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - CROSS-SECTION DATA\n",
    "---\n",
    "#### **1. State the fundamental hypothesis under which the Ordinary Least Squares (OLS) estimators are unbiased.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let $y$ and $x$ be two variables representing some population and, since the goal is to state the dependent variable $y$ in terms of the explanatory variable $x$, we write the simple linear regression model:\n",
    "\n",
    "$$\n",
    "y = \\beta_{0} + \\beta_{1} x + u,\n",
    "$$\n",
    "\n",
    "where the variable $u$ is the disturbance term, standed as well for the unobserved variable, which measures the effect of the change in $y$ with $x$ being unobserved. Moreover, $\\beta_{0}$ is the intercept parameter and $\\beta_{1}$ is called the slope parameter, because, considering $u$ fixed, then $\\Delta u = 0$, and the variation of $y$ is linear with variation of $x$, *i.e.*,\n",
    "\n",
    "$$\n",
    "\\Delta y = \\beta_{1} \\Delta x.\n",
    "$$\n",
    "\n",
    "In order to compute the estimators of $\\beta_{0}$ and $\\beta_{1}$, it is necessary to know the relation beteween the variables $u$ and $x$. To that, we can fist assume that the average alue of $u$ is zero, *i.e.*, \n",
    "\n",
    "$$\n",
    "E(u) = 0.\n",
    "$$\n",
    "\n",
    "Defining the the coditional distribution of $u$ given $x$ and assuming that the average of $u$ does not depend on the value of $x$, we arrive in the following equality:\n",
    "\n",
    "$$\n",
    "E(u|x) = E(u) = 0.\n",
    "$$\n",
    "\n",
    "Finally, considering the equation above and applying the expectation on the simple linear regression model, we obtain:\n",
    "\n",
    "$$\n",
    "E(y|x) = \\beta_{0} +  \\beta_{1} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **2. Show that under this assumption the OLS estimators are indeed unbiased.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to derive the Ordinary Least Squares (OLS) estimators, we consider $n$ samples $(x_{i}, y_{i})$ with $i = 1, \\cdots, n$, described by\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1} x_{i} + u_{i}.\n",
    "$$\n",
    "\n",
    "From the equation above, it is possible to isolate $u_{i}$ and, then, use the fundamental hypothesis compute the estimators $\\hat \\beta_{0}$ and $\\hat \\beta_{1}$, by the method of moments.\n",
    "\n",
    "$$\n",
    "u_{i} = y_{i} - \\beta_{0} - \\beta_{1} x_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(u_{i}) = 0 \\iff E(y_{i} - \\beta_{0} - \\beta_{1} x_{i}) = 0\n",
    "\\iff \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat \\beta_0 - \\hat \\beta_1x_i) = 0 \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Cov(x,u) = E(xu) = 0 \\iff E(x_{i}(y_{i} - \\beta_{0} - \\beta_{1} x_{i})) = 0 \n",
    "\\iff \\frac{1}{n}\\sum_{i=1}^{n}x_i(y_i - \\hat \\beta_0 - \\hat \\beta_1x_i) = 0 \\quad \\quad (2)\n",
    "$$\n",
    "\n",
    "Starting from the equations above, we obtaing the following system of equations, which will be solved for the variables $\\hat \\beta_{0}$ and $\\hat \\beta_{1}$.\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "\\sum_{i=1}^{n}y_i - n \\hat \\beta_0 - \\hat \\beta_1 \\sum_{i=1}^{n} x_i = 0 \\quad \\quad \\quad \\quad \\quad \\quad \\quad (3)\\\\ \n",
    "\\\\\n",
    "\\sum_{i=1}^{n}x_iy_i - \\hat \\beta_0 \\sum_{i=1}^{n} x_{i} - \\hat \\beta_1 \\sum_{i=1}^{n} x_i^{2} = 0 \\quad \\quad \\quad (4)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "From the equation $(3)$, we obtain that \n",
    "\n",
    "$$\n",
    "\\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x}, \\quad \\quad (5)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n}x_{i} \\quad \\text{and} \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n}y_{i}.\n",
    "$$\n",
    "\n",
    "We now substitute the equation $(5)$ in the equation $(2)$, which gives us the following result:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_{i} \\left(y_{i} - (\\bar{y} - \\hat \\beta_{1} \\bar{x}) - \\hat \\beta_{1} x_{i}\\right) = 0\n",
    "\\iff \\sum_{i=1}^{n} x_{i} (y_{i} - \\bar{y}) = \\hat \\beta_{1} \\sum_{i=1}^{n} x_{i} (x_{i} - \\bar{x}) \\quad \\quad (6)\n",
    "$$\n",
    "\n",
    "By using the following results, we can rewrite the equation $(6)$ and, therefore, isolate $\\hat \\beta_{1}$.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_{i} (y_{i} - \\bar{y}) = \\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y}) \\quad \\text{and} \\quad \n",
    "\\sum_{i=1}^{n} x_{i} (x_{i} - \\bar{x}) = \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2} > 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\beta_{1} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}} \\quad \\quad (7)\n",
    "$$\n",
    "\n",
    "In conclusion, the OLS estimators are given  by\n",
    "\n",
    "$$\n",
    "\\hat \\beta_{0} = \\bar{y} - \\hat \\beta_{1} \\bar{x} \\quad \\text{and} \\quad \\hat \\beta_{1} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}},\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the means considering $n$ samples of the independent and dependent variables.\n",
    "\n",
    "Now that the estimators have been computed, the goal is to prove that they are unbiased, by calculating their respective expectations. To that, we rewrite the expression of $\\hat \\beta_{1}$ as follows:\n",
    "\n",
    "First, let $s_{x}^{2} = \\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}$. Then, $\\hat \\beta_{1}$ can we writen as:\n",
    "\n",
    "$$\n",
    "\\hat \\beta_{1} = \\frac{\\sum_{i=1}^{n} y_{i}(x_{i} - \\bar{x})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}} = \n",
    "\\frac{\\sum_{i=1}^{n} (\\beta_{0} + \\beta_{1} x_{i} + u_{i})(x_{i} - \\bar{x})}{s_{x}^{2}} = \n",
    "\\frac{\\beta_{0} \\sum_{i=1}^{n} (x_{i} - \\bar{x}) + \\beta_{1} \\sum_{i=1}^{n} (x_{i} - \\bar{x}) x_{i} + \\sum_{i=1}^{n} (x_{i} - \\bar{x}) u_{i}}{s_{x}^{2}}\n",
    "$$\n",
    "\n",
    "From the expression above, we know that \n",
    "* $\\sum_{i=1}^{n} (x_{i} - \\bar{x}) = 0$\n",
    "\n",
    "* $\\sum_{i=1}^{n} (x_{i} - \\bar{x}) x_{i} = s_{x}^{2}$\n",
    "\n",
    "* $x_{i} - \\bar{x} = d_{i}$\n",
    "\n",
    "It turns out, then, that the slope estimator will be given by:\n",
    "\n",
    "$$\n",
    "\\hat \\beta_{1} = \\frac{\\beta_{1} s_{x}^{2} + \\sum_{i=1}^{n} d_{i} u_{i}}{s_{x}^{2}} = \n",
    "\\beta_{1} + \\frac{1}{s_{x}^{2}} \\sum_{i=1}^{n} d_{i} u_{i}\n",
    "$$\n",
    "\n",
    "Then, the bias of the estimator $\\hat \\beta_{1}$ is computed:\n",
    "\n",
    "$$\n",
    "b(\\beta_{1}, \\hat \\beta_{1}) = E(\\hat \\beta_{1}) - \\beta_{1} = \\beta_{1} + \\left( \\frac{1}{s_{x}^{2}} \\sum_{i=1}^{n} E(d_{i} u_{i}) \\right) - \\beta_{1} = \n",
    "\\frac{1}{s_{x}^{2}} \\sum_{i=1}^{n} d_{i} E(u_{i})\n",
    "$$\n",
    "\n",
    "Given that, by hypothesis, $E(u_{i}) = 0$, then the estimator $\\hat \\beta_{1}$ is unbiased, because $b(\\beta_{1}, \\hat \\beta_{1}) = 0$.\n",
    "\n",
    "We do the same procedure for $\\hat \\beta_{0}$, starting by writing it as a function of $\\bar{x}$ and $\\bar{u}$:\n",
    "\n",
    "$$\n",
    "\\hat \\beta_{0} = (\\beta_{0} + \\beta_{1} \\bar{x} + \\bar{u}) - \\hat \\beta_{1} \\bar{x} = \\beta_{0} + (\\beta_{1} - \\hat \\beta_{1}) \\bar{x} + \\bar{u}\n",
    "$$\n",
    "\n",
    "Finally, the bias of the estimator $\\hat \\beta_{0}$ will be given by:\n",
    "\n",
    "$$\n",
    "b(\\beta_{0}, \\hat \\beta_{0}) = E(\\hat \\beta_{0}) - \\beta_{0} = \\beta_{0} + E\\left((\\beta_{1} - \\hat \\beta_{1}) \\bar{x}\\right) + E(\\bar{u}) - \\beta_{0}\n",
    "$$\n",
    "\n",
    "By hypothesis, $E(u_{i}) = 0$ and, given that it was already proved that $E(\\hat \\beta_{1}) = \\beta_{1}$, then, $E\\left((\\beta_{1} - \\hat \\beta_{1}) \\bar{x}\\right) = 0$.\n",
    "\n",
    "From this, we conclude that $b(\\beta_{0}, \\hat \\beta_{0}) = 0$, which means that the estimator is unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **3. Explain the sample selection bias with an example from the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample selection bias occurs when the OLS estimator is affected by using data resulting from non-random sample selection. In an example involving fertilizer and land quality, seen in the course, the issue arises when experiments aren't entirely randomized. Factors known to the experimenter, such as sunlight exposure or susceptibility to pests, may influence the application of fertilizer, leading to sample selection bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **4. Explain the omitted variable bias with an example from the course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phenomenon happens when there is an exclusion of a relevant variable from the model. Generelly, in this case, the OLS estimators tend to be biased. To prove that, we first consider the true population model, consisting of two explanatory variables and one error term:\n",
    "\n",
    "$$\n",
    "y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + u.\n",
    "$$\n",
    "\n",
    "If we perform a simple linear regression of $y$ on $x_{1}$ exclusively, *i.e.*, we ignore $x_{2}$, we obtain an underspecified model, given by:\n",
    "\n",
    "$$\n",
    "\\widetilde{y} = \\widetilde{\\beta_{0}} + \\widetilde{\\beta_{1}} x_{1}\n",
    "$$\n",
    "\n",
    "If we derive an expression for $\\widetilde{\\beta_{1}}$, we will otain the a similar equation as $(7)$, as follows:\n",
    "\n",
    "$$\n",
    "\\widetilde{\\beta_{1}} = \\frac{\\sum_{i=1}^{n} y_{i}(x_{i1} - \\bar{x}_{1})}{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1})^{2}} \\quad \\quad \\quad \\quad (8)\n",
    "$$\n",
    "\n",
    "Next, considering the true model, for each observation $i$, we can write\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + u_{i} \\quad \\quad (9)\n",
    "$$\n",
    "\n",
    "Substituting $(9)$ in $(8)$, the numerator of $(8)$ will be given by:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (\\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + u_{i})(x_{i1} - \\bar{x}_{1}) = \n",
    "\\beta_{1} \\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1})^{2} + \\beta_{2} \\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1}) x_{i2} + \\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1}) u_{i} \\quad \\quad (10)\n",
    "$$\n",
    "\n",
    "With $(10)$, the equation $(8)$ can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\widetilde{\\beta_{1}} = \\beta_{1} + \\beta_{2} \\frac{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1}) x_{i2}}{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1})^{2}} + \\frac{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1}) u_{i}}{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1})^{2}} \\quad \\quad (11)\n",
    "$$\n",
    "\n",
    "Finally, using the fact that $E(u_{i}) = 0$, we take the expectation of the estimator $\\widetilde{\\beta_{1}}$:\n",
    "\n",
    "$$\n",
    "E(\\widetilde{\\beta_{1}}) = \\beta_{1} + \\beta_{2} \\frac{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1}) x_{i2}}{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_{1})^{2}} \\neq \\beta_{1},\n",
    "$$\n",
    "\n",
    "which leads to the fact that $\\widetilde{\\beta_{1}}$ is biased.\n",
    "\n",
    "One example seen in the course, was the following model:\n",
    "\n",
    "$$\n",
    "wage = \\beta_{0} + \\beta_{1} education + \\beta_{2} ability + u\n",
    "$$\n",
    "\n",
    "The bias of the estimator of $\\beta_{1}$ was verified after having underspecified the model by running a simple linear regression of $wage$ on $education$, despising the $ability$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **5. Explain the problem of multicollinearity. Is it a problem in this dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue of multicollinearity arises when two or more columns of the matrix $ X $ are linearly dependent (or nearly so).\n",
    "\n",
    "If this is the case, the determinant of the matrix $X^{\\prime}X$ becomes zero or nearly zero.\n",
    "\n",
    "$$\n",
    "det(X^\\prime X) = 0 \n",
    "$$\n",
    "\n",
    "In such instances, the matrix $ (X^\\prime X) $ is non-invertible, rendering the computation of the OLS estimator impossible.\n",
    "\n",
    "A potential remedy for this problem involves either removing one of the linearly dependent columns or introducing additional observations to the dataset.\n",
    "\n",
    "Fortunately, the dataset for this project does not exhibit multicollinearity issues since the determinant of the matrix $ (X^\\prime X) $ is non-zero, as evidenced in the code below:\n",
    "\n",
    "TO DO: talk about R-square influence in multicollineartiy (page 95), talk about VIF = 1 / (1 - R-square)\n",
    "TO DO: discuss that the \"importance\" on multicollinearity depends on how big beta_i is compared to its std_error. A high multicollinearity leads into high Var(beta_i), what becomes a problem depending on the magnitude of beta_i\n",
    "\n",
    "Wooldridge says: \"[...] for statistical inference, what ultimately matters is how big beta_j is in relation to its standard deviation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, f\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "raw_data_path = 'textfiles/HPRICE2.raw'\n",
    "labels_path = 'textfiles/HPRICE2.DES'\n",
    "\n",
    "raw_data = np.loadtxt(raw_data_path)\n",
    "\n",
    "# TO DO: read \\n in the labels file\n",
    "with open(labels_path, 'r') as file:\n",
    "    file.readline()\n",
    "    file.readline()\n",
    "\n",
    "    labels_line = file.readline().strip()\n",
    "    labels = np.array(labels_line.split())\n",
    "\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=labels)\n",
    "\n",
    "Y = df['price']\n",
    "X = df.drop('price', axis=1)\n",
    "\n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = X.columns \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] \n",
    "# Exibir os resultados\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **6. Create three categories of nox levels (low, medium, high), corresponding to the following percentiles: 0-25%, 26%-74%, 75%-100%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nox_percentiles_threshholds = np.percentile(df['nox'], [25, 50, 75])\n",
    "nox_categories = np.digitize(df['nox'], nox_percentiles_threshholds)\n",
    "\n",
    "df['nox_category'] = nox_categories\n",
    "\n",
    "low_nox = df[df['nox_category'] == 0]\n",
    "medium_nox = df[df['nox_category'] == 1]\n",
    "high_nox = df[df['nox_category'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **7. Compute for each category of nox level the average median price and comment on your\n",
    "results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Price means')\n",
    "print('Low NOx: ', round(low_nox['price'].mean(), 2))\n",
    "print('Medium NOx: ', round(medium_nox['price'].mean(), 2))\n",
    "print('High NOx: ', round(high_nox['price'].mean(), 2))\n",
    "\n",
    "# TO DO: comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **8. Produce a scatter plot with the variable price on the y-axis and the variable nox on the x-axis. Is this a ceteris paribus effect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['nox'], df['price'])\n",
    "plt.xlabel('nox')\n",
    "plt.ylabel('price')\n",
    "plt.title('Scatter Plot: nox vs price')\n",
    "plt.show()\n",
    "\n",
    "# TO DO: comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **9. Run a regression of price on a constant, crime, nox, rooms, proptax. Comment on the histogram of the residuals. Interpret all coefficients.**\n",
    "\n",
    "Interpretations\n",
    "\n",
    "**All constant (intercept)**\n",
    "- **Coefficient:** -18,680\n",
    "- **Interpretation:** When all independent variables (crime, nox, rooms, proptax) are zero, the predicted value of the dependent variable (price) is -18,680. This may not have much sense.\n",
    "\n",
    "**Variable \"crime\":**\n",
    "- **Coefficient:** -136.5438\n",
    "- **Interpretation:** Holding other variables constant, a one-unit increase in the \"crime\" variable is associated with a decrease in the predicted value of the price by 136.5438 units.\n",
    "\n",
    "**Variable \"nox\":**\n",
    "- **Coefficient:** -660.4672\n",
    "- **Interpretation:** Holding other variables constant, a one-unit increase in the \"nox\" variable is associated with a decrease in the predicted value of the price by 660.4672 units.\n",
    "\n",
    "**Variable \"rooms\":**\n",
    "- **Coefficient:** 7797.9286\n",
    "- **Interpretation:** Holding other variables constant, a one-unit increase in the \"rooms\" variable is associated with an increase in the predicted value of the price by 7797.9286 units.\n",
    "\n",
    "**Variable \"proptax\":**\n",
    "- **Coefficient:** -89.4144\n",
    "- **Interpretation:** Holding other variables constant, a one-unit increase in the \"proptax\" variable is associated with a decrease in the predicted value of the price by 89.4144 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the independent variables\n",
    "X = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "X = sm.add_constant(X)  # Add a constant column\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df['price']\n",
    "\n",
    "# Fit the regression model\n",
    "model_fit_q9 = sm.OLS(y, X).fit()\n",
    "residuals = model_fit_q9.resid\n",
    "\n",
    "print('Question 9')\n",
    "print('Var(u|X) = ', round(np.var(residuals), 2))\n",
    "print(model_fit_q9.summary())\n",
    "\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# TO DO: comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: talk about big variance of u\n",
    "\n",
    "Interpretations\n",
    "\n",
    "**All constant (intercept)**: -18,680\n",
    "- When all independent variables (crime, nox, rooms, proptax) are zero, the predicted value of the dependent variable (price) is -18,680. This may not have much sense.\n",
    "\n",
    "**Variable \"crime\":**: -136.5438\n",
    "- Holding other variables constant, a one-unit increase in the \"crime\" variable is associated with a decrease in the predicted value of the price by 136.5438 units.\n",
    "\n",
    "**Variable \"nox\":**: -660.4672\n",
    "- Holding other variables constant, a one-unit increase in the \"nox\" variable is associated with a decrease in the predicted value of the price by 660.4672 units.\n",
    "\n",
    "**Variable \"rooms\":**: 7797.9286\n",
    "- Holding other variables constant, a one-unit increase in the \"rooms\" variable is associated with an increase in the predicted value of the price by 7797.9286 units.\n",
    "\n",
    "**Variable \"proptax\":**: -89.4144\n",
    "- Holding other variables constant, a one-unit increase in the \"proptax\" variable is associated with a decrease in the predicted value of the price by 89.4144 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **10. Run a regression of lprice on a constant, crime, nox, rooms, proptax. Comment on the histogram of the residuals. Interpret all coefficients.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the independent variables\n",
    "X = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "X = sm.add_constant(X)  # Add a constant column\n",
    "\n",
    "# Define the dependent variable\n",
    "Y = df['lprice']\n",
    "\n",
    "# Fit the regression model\n",
    "model_fit_q10 = sm.OLS(Y, X).fit()\n",
    "residuals = model_fit_q10.resid\n",
    "\n",
    "print('Question 10')\n",
    "print('Var(u|X) = ', round(np.var(residuals), 2))\n",
    "print(model_fit_q10.summary())\n",
    "\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretations\n",
    "\n",
    "**All constant (intercept)**: 8.6550\n",
    "- When all independent variables (crime, nox, rooms, proptax) are zero, the predicted value of the price is 8.6550.\n",
    "\n",
    "**Variable \"crime\"**: -0.0125\n",
    "- Holding other variables constant, a one-unit increase in the \"crime\" variable is associated with a decrease in the predicted value of the price by 0.0125 units.\n",
    "\n",
    "**Variable \"nox\"**: -0.0476\n",
    "- Holding other variables constant, a one-unit increase in the \"nox\" variable is associated with a decrease in the predicted value of the price by 0.0476 units.\n",
    "\n",
    "**Variable \"rooms\"**: 0.2816\n",
    "- Holding other variables constant, a one-unit increase in the \"rooms\" variable is associated with an increase in the predicted value of the price by 0.2816 units.\n",
    "\n",
    "**Variable \"proptax\"**: -0.0043\n",
    "- Holding other variables constant, a one-unit increase in the \"proptax\" variable is associated with a decrease in the predicted value of the price by 0.0043 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **11. Run a regression of lprice on a constant, crime, lnox, rooms, lproptax. Comment on the histogram of the residuals. Interpret all coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the independent variables\n",
    "X = df[['crime', 'lnox', 'rooms', 'proptax']]\n",
    "X = sm.add_constant(X)  # Add a constant column\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df['lprice']\n",
    "\n",
    "# Fit the regression model\n",
    "model_fit_q11 = sm.OLS(y, X).fit()\n",
    "residuals = model_fit_q11.resid\n",
    "\n",
    "print('Question 11')\n",
    "print('Var(u|X) = ', round(np.var(residuals), 2))\n",
    "print(model_fit_q11.summary())\n",
    "\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All constant (intercept)**: 8.8553\n",
    "- When all independent variables (crime, nox, rooms, proptax) are zero, the predicted value of the price is 8.8553.\n",
    "\n",
    "**Variable \"crime\"**: -0.0125\n",
    "- Holding other variables constant, a one-unit increase in the \"crime\" variable is associated with a decrease in the predicted value of the price by 0.0125 units.\n",
    "\n",
    "**Variable \"nox\"**: -0.0476\n",
    "- Holding other variables constant, a one-unit increase in the \"nox\" variable is associated with a decrease in the predicted value of the price by 0.0476 units.\n",
    "\n",
    "**Variable \"rooms\"**: 0.2816\n",
    "- Holding other variables constant, a one-unit increase in the \"rooms\" variable is associated with an increase in the predicted value of the price by 0.2816 units.\n",
    "\n",
    "**Variable \"proptax\"**: -0.0042\n",
    "- Holding other variables constant, a one-unit increase in the \"proptax\" variable is associated with a decrease in the predicted value of the price by 0.0042 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **12. In the specification of question 10, test the hypothesis H0: $\\beta_{nox}$ <0 vs. H1: $\\beta_{nox}$ > 0 at the 1% level.**\n",
    "\n",
    "TO DO: add explanation\n",
    "To DO: mention ': Given the observed value of the t statistic, what is the smallest\n",
    "significance level at which the null hypothesis would be rejected? This level is known\n",
    "as the p-value for the test'\n",
    "\n",
    "' In order\n",
    "to compute p-values, we either need extremely detailed printed tables of the t distribution—which is not very practical—or a computer program that computes areas\n",
    "under the probability density function of the t distribution'\n",
    "\n",
    "' If a regression package reports a\n",
    "p-value along with the standard OLS output, it is almost certainly the p-value for testing the null hypothesis H0: \u0001j  0 against the two-sided alternative.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information from the model\n",
    "beta_nox = model_fit_q10.params['nox']\n",
    "se_nox = model_fit_q10.bse['nox']\n",
    "\n",
    "# t-statistic\n",
    "t_value = beta_nox / se_nox\n",
    "deg_freedom = model_fit_q10.df_resid\n",
    "\n",
    "# One-sided p-value\n",
    "p_value_one_tailed = 1 - t.cdf(abs(t_value), deg_freedom)\n",
    "alpha = 0.01\n",
    "\n",
    "# Compare the p-value with the significance level\n",
    "if p_value_one_tailed < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value_one_tailed:.7f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value_one_tailed:.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **13. In the specification of question 10, test the hypothesis H0: $\\beta_{nox}$ = 0 vs. H1: $\\beta_{nox}$ ≠ 0 at the 1% level using the p-value of the test.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information from the model\n",
    "beta_nox = model_fit_q10.params['nox']\n",
    "se_nox = model_fit_q10.bse['nox']\n",
    "\n",
    "# t-statistic\n",
    "t_value = beta_nox / se_nox\n",
    "\n",
    "# Two-sided p-value\n",
    "p_value_two_tailed = 2 * (1 - t.cdf(abs(t_value), deg_freedom))\n",
    "alpha = 0.01\n",
    "\n",
    "# Compare the p-value with the significance level\n",
    "if p_value_two_tailed < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value_two_tailed:.7f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value_two_tailed:.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **14. In the specification of question 10, test the hypothesis H0: $\\beta_{crime}$ = $\\beta_{proptax}$ at the 10% level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information from the model\n",
    "beta_crime = model_fit_q10.params['crime']\n",
    "beta_proptax = model_fit_q10.params['proptax']\n",
    "se_crime = model_fit_q10.bse['crime']\n",
    "se_proptax = model_fit_q10.bse['proptax']\n",
    "\n",
    "# Get covariance matrix\n",
    "cov_matrix = model_fit_q10.cov_params()\n",
    "cov_crime_proptax = cov_matrix.loc['crime', 'proptax']\n",
    "\n",
    "# Sqrt of the sum of squared standard errors minus 2 times the covariance\n",
    "se_crime_minus_proptax = np.sqrt(se_crime**2 + se_proptax**2 - 2 * cov_crime_proptax)\n",
    "\n",
    "# t-statistic\n",
    "t_value = (beta_crime - beta_proptax) / se_crime_minus_proptax\n",
    "deg_freedom = model_fit_q10.df_resid\n",
    "\n",
    "# Two-sided p-value\n",
    "p_value = 2 * (1 - t.cdf(abs(t_value), deg_freedom))\n",
    "alpha = 0.1\n",
    "\n",
    "# Compare the p-value with the significance level\n",
    "if p_value < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value:.7f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value:.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **15. In the specification of question 10, test the hypothesis H0: $\\beta_{crime}$ = 0, $\\beta_{proptax}$ = 0 at the 10% level.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Define X unrestricted and restricted\n",
    "Xur = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "Xur = sm.add_constant(Xur)\n",
    "\n",
    "Xr = df[['nox', 'rooms']]\n",
    "Xr = sm.add_constant(Xr)\n",
    "\n",
    "Y = df['lprice']\n",
    "\n",
    "modelUr = sm.OLS(Y, Xur).fit()\n",
    "modelR = sm.OLS(Y, Xr).fit()\n",
    "\n",
    "SSRur = modelUr.ssr\n",
    "SSRr = modelR.ssr\n",
    "\n",
    "alpha = 0.1\n",
    "q = modelR.df_resid - modelUr.df_resid\n",
    "n_k_1 = modelUr.df_resid\n",
    "\n",
    "F = ((SSRr - SSRur) / q) / (SSRur / n_k_1)\n",
    "man_p_value = 2 * (1 - f.cdf(F, q, n_k_1))\n",
    "print(f'F-statistic: {man_p_value:.30f}') \"\"\"\n",
    "\n",
    "alpha = 0.1\n",
    "hypothesis = '(proptax = 0), (nox = 0)'\n",
    "\n",
    "p_value = model_fit_q10.f_test(hypothesis).pvalue\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value:.30f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value:.30f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **16. In the specification of question 10, test the hypothesis H0: $\\beta_{crime}$ = -500, $\\beta_{proptax}$ = -100 at the 10% level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Define X unrestricted and restricted\n",
    "Xur = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "Xur = sm.add_constant(Xur)\n",
    "\n",
    "Xr = df[['nox', 'rooms']]\n",
    "Xr = sm.add_constant(Xr)\n",
    "\n",
    "Yur = df['lprice']\n",
    "Yr = df['lprice'] - 500*df['crime'] - 100*df['proptax']\n",
    "\n",
    "modelUr = sm.OLS(Yur, Xur).fit()\n",
    "modelR = sm.OLS(Yr, Xr).fit()\n",
    "\n",
    "SSRur = modelUr.ssr\n",
    "SSRr = modelR.ssr\n",
    "\n",
    "alpha = 0.1\n",
    "q = modelR.df_resid - modelUr.df_resid\n",
    "n_k_1 = modelUr.df_resid\n",
    "\n",
    "F = ((SSRr - SSRur) / q) / (SSRur / n_k_1)\n",
    "p_value = 1 - f.cdf(F, q, n_k_1) \"\"\"\n",
    "\n",
    "alpha = 0.1\n",
    "hypothesis = '(proptax = -100), (nox = -500)'\n",
    "\n",
    "p_value = model_fit_q10.f_test(hypothesis).pvalue\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value:.30f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value:.30f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **17. In the specification of question 10, test the hypothesis H0: $\\beta_{crime}$ + $\\beta_{proptax}$ = -1000 at the 10% level.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Define X unrestricted and restricted\n",
    "Xur = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "Xur = sm.add_constant(Xur)\n",
    "\n",
    "Xr_ = df[['nox', 'rooms']]\n",
    "Xr_ = sm.add_constant(Xr_)\n",
    "\n",
    "Yur = df['lprice']\n",
    "Yr = df['lprice'] - 1000*(df['crime'] + df['proptax'])\n",
    "\n",
    "modelUr = sm.OLS(Yur, Xur).fit()\n",
    "modelR = sm.OLS(Yr, Xr).fit()\n",
    "\n",
    "SSRur = modelUr.ssr\n",
    "SSRr = modelR.ssr\n",
    "\n",
    "alpha = 0.1\n",
    "q = modelR.df_resid - modelUr.df_resid\n",
    "n_k_1 = modelUr.df_resid\n",
    "\n",
    "F = ((SSRr - SSRur) / q) / (SSRur / n_k_1)\n",
    "p_value = 1 - f.cdf(F, q, n_k_1) \"\"\"\n",
    "\n",
    "alpha = 0.1\n",
    "hypothesis = '(proptax + nox = -1000)'\n",
    "\n",
    "p_value = model_fit_q10.f_test(hypothesis).pvalue\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value:.30f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value:.30f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **18. In the specification of question 10, test the hypothesis that all coefficients are the same for observations with low levels of nox vs. medium and high levels of nox.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "low_nox = df[df['nox_category'] == 0]\n",
    "medium_high_nox = df[df['nox_category'] != 0]\n",
    "\n",
    "X_low = low_nox[['crime', 'nox', 'rooms', 'proptax']]\n",
    "X_low = sm.add_constant(X_low)\n",
    "Y_low = low_nox['lprice']\n",
    "\n",
    "X_medium_high = medium_high_nox[['crime', 'nox', 'rooms', 'proptax']]\n",
    "X_medium_high = sm.add_constant(X_medium_high)\n",
    "Y_medium_high = medium_high_nox['lprice']\n",
    "\n",
    "model_fit_low = sm.OLS(Y_low, X_low).fit()\n",
    "model_fit_medium_high = sm.OLS(Y_medium_high, X_medium_high).fit()\n",
    "\n",
    "#Get information from the model\n",
    "betas_low = model_fit_low.params\n",
    "betas_medium_high = model_fit_medium_high.params\n",
    "ses_low = model_fit_low.bse\n",
    "ses_medium_high = model_fit_medium_high.bse\n",
    "\n",
    "\"\"\" # Get covariance matrix\n",
    "cov_matrix = model_fit_q10.cov_params()\n",
    "cov_crime_proptax = cov_matrix.loc['crime', 'proptax']\n",
    "\n",
    "# Sqrt of the sum of squared standard errors minus 2 times the covariance\n",
    "se_crime_minus_proptax = np.sqrt(se_crime**2 + se_proptax**2 - 2 * cov_crime_proptax)\"\"\"\n",
    "\n",
    "# t-statistic\n",
    "t_value = (betas_low - betas_medium_high) / ((ses_low**2 + ses_medium_high**2)**0.5)\n",
    "deg_freedom = model_fit_low.df_resid\n",
    "\n",
    "# Two-sided p-value\n",
    "p_values = 2 * (1 - t.cdf(abs(t_value), deg_freedom))\n",
    "alpha = 0.1\n",
    "\n",
    "results = pd.DataFrame({'t_value': t_value, 'p_value': p_values}, index=model_fit_low.params.index)\n",
    "results['significant'] = results['p_value'] < alpha\n",
    "\n",
    "\"\"\" # Compare the p-value with the significance level\n",
    "if p_value < alpha:\n",
    "    print(f'Reject the null hypothesis. p-value: {p_value:.7f}')\n",
    "else:\n",
    "    print(\n",
    "        f'Fail to reject the null hypothesis. p-value: {p_value:.7f}') \"\"\"\n",
    "        \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **19. Repeat the test of question 18 but now assuming that only the coefficients of nox and proptax can change between the two groups of observations. State and test H0.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - HETEROSKEDASTICITY \n",
    "---\n",
    "#### **20. Explain the problem of heteroskedasticity with an example of the course.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, under homoskedasticity, the variances of the error term $u$ and the dependent variable $y$ are assumed to be constant, *i.e.*, \n",
    "\n",
    "$$\n",
    "Var(u|x) = Var(y|x) = \\sigma^{2}\n",
    "$$\n",
    "\n",
    "However, under heteroskedasticity conditions, what happens is that the variance of the error $Var(u|x)$ depends on the independent variable $x$. Consequently, the variance of $y$ $Var(y|x)$ will also depend on $x$. This violation of the homoskedasticity assumption can lead to issues, such as biased standard errors and inefficient parameter estimates in Ordinary Least Squares regression.\n",
    "\n",
    "In the example studied in course, the examination of the education-wage relationship underscores the challenge posed by heteroskedasticity. The quest for an unbiased estimation of education's impact on wages necessitates the assumption $E(u|educ) = 0$, accompanied by the assumption of homoskedasticity $Var(u∣educ) = \\sigma^{2}$. This implies constant wage variance $Var(wage∣educ) = \\sigma^{2}$ across education levels, allowing for varying mean wages while assuming consistent variance. However, the realism concern is acknowledged: higher education levels may introduce greater wage variability due to diverse job opportunities, contrasting with lower variability at lower education levels.\n",
    "\n",
    "Expanding on this, assume we build a model assuming homoskedasticity, implying constant error term variance across education levels. In the real world, individuals with higher education may experience more diverse work opportunities, leading to increased wage variability. Conversely, those with lower education levels may face fewer opportunities, resulting in reduced wage variability. Thus, insisting on homoskedasticity introduces bias into the model, as it overlooks the varying wage volatility associated with different education levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **21. Suppose that $ E(u u')= \\sigma^2 \\Omega$. Show that the GLS estimator is the best linear unbiased estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fistly, considering the model $y = X \\beta + u$, where $E(u|X) = 0$, we derive the expression of the OLS estimator of $\\beta$, called $b$. Considering a set of $n$ i.i.d. observations, the collected data is represented by the following esquation:\n",
    "\n",
    "$$\n",
    "y = X b + u \\quad \\quad \\quad \\quad (12)\n",
    "$$\n",
    "\n",
    "By expliciting the matrix notation of the equation (12), we obtain the following equivalent equation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{N}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1K}\\\\ \n",
    "x_{21} & x_{21} & \\cdots & x_{2K}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "x_{N1} & x_{N2} & \\cdots & x_{NK} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "b_{K}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "u_{1}\\\\\n",
    "u_{2}\\\\\n",
    "\\vdots\\\\\n",
    "u_{N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to derive an expression for $b$, we minimize sum of the squared residuals, i.e., we minimize\n",
    "\n",
    "$$\n",
    "u' u = \\begin{bmatrix}\n",
    "u_{1}\\ u_{2}\\ \\cdots\\ u_{N}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "u_{1}\\\\\n",
    "u_{2}\\\\\n",
    "\\vdots\\\\\n",
    "u_{N}\n",
    "\\end{bmatrix}= \\sum_{i=1}^{N} u_{i}^{2}\n",
    "$$\n",
    "\n",
    "To that, it is first necessary to isolate the expression of $u$ and then, compute its transpose, so that the product $u' u$ can be minimized. From the equation (12), is given by:\n",
    "\n",
    "$$\n",
    "u = y - X b \\Rightarrow u' = (y - X b)' = y' - b' X'\n",
    "$$\n",
    "\n",
    "Thus, the product to be minimized is:\n",
    "\n",
    "$$\n",
    "u' u = (y' - b' X')(y - X b) = y'y - y'Xb - b'X'y + b'X'Xb \n",
    "$$\n",
    "\n",
    "By noting that $b'X'y = (b'X'y)' = y'Xb$, we arrive in the following expression:\n",
    "\n",
    "$$\n",
    "\\text{min } u'u = y'y - 2b'X'y + b'X'Xb \\quad \\quad (13)\n",
    "$$\n",
    "\n",
    "Finally, what is necessary to do is to take the partial derivative of the equation $(13)$ with respect to $b$ and solve it when it is equal to zero. To that, we use the two following results:\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "\\frac{\\partial}{\\partial b} (b'X'y) = X'y \\\\ \n",
    "\\frac{\\partial}{\\partial b} (b'X'Xb) = 2X'Xb \n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "With the above, the equation to be solved for $b$ will be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} (u'u) = 0 \\iff -2X'y + 2X'Xb = 0 \\iff X'Xb = X'y \\iff b = (X'X)^{-1}X'y \\quad (14)\n",
    "$$\n",
    "\n",
    "Now that the estimator has been computed as shown in the equation $(14)$, to shown that it is the best linear unbiased estimator (BLUE), we first check that it is linear. Indeed, the linearity is checked immediately by seeing the linear relatioship between $b$ and the dependent variable $y$. The second step is to show its unbiasedness. To that, we compute its expected value.\n",
    "\n",
    "$$\n",
    "E(b) = E\\left((X'X)^{-1}X'y\\right) = (X'X)^{-1}X'E(y) \\quad \\quad (15)\n",
    "$$\n",
    "\n",
    "Since the assumption $E(u|X) = 0$ holds, then $E(y) = X \\beta$ and the equation $(15)$ can be written as follows, proving that the estimator $b$ is unbiased.\n",
    "\n",
    "$$\n",
    "E(b) = (X'X)^{-1}(X'X) \\beta \\iff E(b) = \\beta\n",
    "$$\n",
    "\n",
    "Now, for the next steps, it is useful to rewrite the expression of $b$ as follows:\n",
    "\n",
    "$$\n",
    "\\left.\\begin{matrix}\n",
    "y = X \\beta + u \\\\ \n",
    "b = (X'X)^{-1}X'y\n",
    "\\end{matrix}\\right\\}\\Rightarrow \n",
    "b = \\beta + (X'X)^{-1}X'u \\quad \\quad (16)\n",
    "$$\n",
    "\n",
    "With the equation $(16)$, we compute the variance of the estimator considering the Heteroskedasticity supposition that $Var(u) = E(uu') = \\sigma^{2}\\Omega$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var(b) &= Var\\left(\\beta + (X'X)^{-1}X'u\\right) \\\\\n",
    "       &= Var\\left((X'X)^{-1}X'u\\right) \\\\\n",
    "       &= (X'X)^{-1}X' Var(u) X(X'X)^{-1} \\\\\n",
    "       &= (X'X)^{-1}X' \\sigma^{2} \\Omega X(X'X)^{-1}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, the equation $(17)$ contains the result of the variance of the estimator, considering Heteroskedasticity.\n",
    "\n",
    "$$\n",
    "Var(b) = \\sigma^{2} (X'X)^{-1}X' \\Omega X(X'X)^{-1} \\quad \\quad (17)\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **22. In the specification of question 10, test the hypothesis of no heteroskedasticity of linear form, i.e. in the regression of u2 on constant, crime, nox, rooms, proptax, test H0: crime, nox, room, proptax = 0, where the coefficients k (k = crime, nox, rooms, proptax) are associated with the corresponding explanatory variables.** \n",
    "\n",
    "Answer: We can use the Breusch-Pagan test from the statsmodels library in Python to test for heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = het_breuschpagan(model_fit_q10.resid, model_fit_q10.model.exog)\n",
    "\n",
    "labels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "\n",
    "print(dict(zip(labels, bp_test)))\n",
    "\n",
    "hypothesis = '(proptax = 0), (nox = 0), (rooms = 0), (crime = 0)'\n",
    "\n",
    "p_value = model_fit_q10.f_test(hypothesis).pvalue\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **23. In the specification of question 11, test the hypothesis of no heteroskedasticity of linear form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = het_breuschpagan(model_fit_q11.resid, model_fit_q11.model.exog)\n",
    "\n",
    "labels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "\n",
    "print(dict(zip(labels, bp_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **24.  In the specification of question 12, test the hypothesis of no heteroskedasticity of linear form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = het_breuschpagan(model_fit_q10.resid, model_fit_q10.model.exog)\n",
    "\n",
    "labels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "\n",
    "print(dict(zip(labels, bp_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **25. Comment on the differences between your results of questions 22,23, 24.**\n",
    "\n",
    "TO DO: complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **26. Regardless of the results of the test of question 22, identify the most significant variable causing heteroskedasticity using the student statistics and run a WLS regression with the identified variable as weight.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 - TIME SERIES DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **27. Define strict and weak stationarity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Strict stationarity:** A time series process is considered strictly stationary if the joint probability distribution of its observations remains invariant under shifts in time. In other words, for any set of time points, the entire probability distribution of the data, including the mean, variance, and higher-order moments, remains constant. This implies that all statistical properties of the time series are unchanged over time, making strict stationarity a more stringent condition compared to covariance stationarity, for example.\n",
    "\n",
    "\n",
    "* **Weak stationarity:** A time series process is said to be weak stationary if it presents constants mean, variance and autocorrelation structutre over time, but not its entire probability distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
